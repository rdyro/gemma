{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "likVQiEEYS5X"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# to observe the actual memory getting somewhat conservatively allocated\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "import jax\n",
        "from jax.sharding import PartitionSpec as P\n",
        "from flax.linen import logical_to_mesh_sharding\n",
        "from flax import nnx\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "import sentencepiece as spm\n",
        "import kagglehub\n",
        "#kagglehub.login() # you might need to log in\n",
        "\n",
        "cpu_device, compute_devices = jax.devices(\"cpu\")[0], jax.devices(\"cuda\")\n",
        "jax.config.update(\"jax_compilation_cache_dir\", \n",
        "                  str(Path(\"~/.cache/jax_compilation_cache\").expanduser()))\n",
        "sampler_logger = logging.getLogger(sampler_lib.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n",
            "/home/rdyro/.cache/kagglehub/models/google/gemma-2/flax/gemma2-2b-it/1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# variants v1 have gemma/Flax\n",
        "# variant = '2b-it' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "# weights_dir = kagglehub.model_download(f'google/gemma/Flax/{variant}')\n",
        "\n",
        "variant = \"gemma2-2b-it\"\n",
        "weights_dir = kagglehub.model_download(f\"google/gemma-2/flax/{variant}\")\n",
        "print(weights_dir)\n",
        "ckpt_path = os.path.join(weights_dir, variant)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')\n",
        "(vocab := spm.SentencePieceProcessor()).Load(vocab_path)\n",
        "#vocab.Load(vocab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "57nMYQ4HESaN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gemma.transformer:You did not provide the rngs, setting rngs to nnx.Rngs(0)\n",
            "WARNING:gemma.transformer:You did not provide the rngs, setting rngs to nnx.Rngs(0)\n"
          ]
        }
      ],
      "source": [
        "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=128)\n",
        "graphdef, state_ = jax.eval_shape(lambda: nnx.split(transformer_lib.Transformer(config)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh = jax.sharding.Mesh(compute_devices, (\"x\",))\n",
        "is_param = lambda x: isinstance(x, nnx.VariableState)\n",
        "model_parallel_rules = {\n",
        "  None: None, \n",
        "  \"batch\": None,\n",
        "  \"sequence\": None,\n",
        "  \"vocab\": \"x\", \n",
        "  \"features\": \"x\",\n",
        "  \"q_heads\": \"x\", \n",
        "  \"kv_heads\": \"x\", \n",
        "  \"head_dim\": None, \n",
        "  \"ffw\": \"x\",\n",
        "  \"act_batch\": None,\n",
        "  \"act_sequence\": None,\n",
        "  \"act_heads\": None,\n",
        "  \"act_kv_heads\": None,\n",
        "  \"act_head_dim\": None,\n",
        "}\n",
        "rules = list(model_parallel_rules.items())\n",
        "state_shardings = jax.tree.map(lambda x: logical_to_mesh_sharding(\n",
        "  P(*x.names), mesh, rules), state_, is_leaf=is_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
        "  params = params_lib.load_and_format_params(ckpt_path)\n",
        "  params = params[\"transformer\"]\n",
        "  shardings_flat = jax.tree.leaves(state_shardings)\n",
        "  params_flat = jax.jit(lambda x: jax.tree.leaves(x), \n",
        "                        out_shardings=shardings_flat)(params)\n",
        "\n",
        "state = jax.tree.unflatten(jax.tree.structure(state_), params_flat)\n",
        "transformer = nnx.merge(graphdef, state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampler = sampler_lib.Sampler(transformer, vocab, mesh, rules)\n",
        "# we could also ommit the mesh and the rules, relying on param sharding alone\n",
        "# otherwise: sampler = sampler_lib.Sampler(transformer, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:gemma.sampler:Prefill took 2.8600e-02 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:gemma.sampler:Initialization took 3.5512e-01 s\n",
            "DEBUG:gemma.sampler:AR Sampling took 4.1341e-01 s\n",
            "DEBUG:gemma.sampler:Throughput: tok / sec: 72.567 for sampled_steps = 30\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the spaceship go to the therapist? \n",
            "\n",
            "Because it had a lot of \"space\" issues! ðŸ‘½ðŸš€ðŸš€ \n",
            "<end_of_turn>\n",
            "--------------------------------------------------------------------------------\n",
            "Hi there! ðŸ‘‹  What can I do for you today? ðŸ˜Š \n",
            "<end_of_turn>\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "sampler_logger.setLevel(\"WARNING\")\n",
        "for i in range(1):\n",
        "  sampler_logger.setLevel(\"DEBUG\")\n",
        "  input_lines = [\"tell a joke that related to sci-fi\", \"hi\"]\n",
        "  out = sampler(input_lines, 128, apply_chat_template=True, echo=False)\n",
        "for line in out.text:\n",
        "  print(line)\n",
        "  print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "gemma",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
