{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "likVQiEEYS5X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['XLA_PYTHON_CLIENT_PREALLOCATE = false']\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import functools\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import random\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "from flax import linen as nn\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "import sentencepiece as spm\n",
        "import kagglehub\n",
        "#kagglehub.login()\n",
        "\n",
        "cpu_device, devices = jax.devices(\"cpu\")[0], jax.devices(\"cuda\")\n",
        "pprint([f\"{k} = {v}\" for k, v in os.environ.items() if k.startswith(\"XLA\")])\n",
        "\n",
        "jax.config.update(\"jax_compilation_cache_dir\", \n",
        "                  str(Path(\"~/.cache/jax_compilation_cache\").expanduser()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "variant = '2b-it' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{variant}')\n",
        "ckpt_path = os.path.join(weights_dir, variant)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.Load(vocab_path)\n",
        "PAD_ID = vocab.pad_id()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums=(0, 1))\n",
        "def casual_attention_mask(seq_len: int, max_seq_len: int) -> jax.Array:\n",
        "  return jnp.arange(seq_len)[..., None] >= jnp.arange(max_seq_len)[None, ...]\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def construct_positions_and_attn_mask(input: jax.Array, max_len: int, \n",
        "                                      pad_id: int = PAD_ID\n",
        "                                      ) -> tuple[jax.Array, jax.Array]:\n",
        "  assert input.ndim == 2 and input.shape[-1] <= max_len\n",
        "  input_len = input.shape[-1]\n",
        "  input = input.astype(jnp.int32)\n",
        "  input_mask = input != pad_id\n",
        "  # positions are zero-indexed, cumsum gives one-indexed values\n",
        "  positions = ((jnp.cumsum(input_mask, axis=-1, dtype=jnp.int32) - 1) \n",
        "               * input_mask)\n",
        "  attention_mask = casual_attention_mask(input_len, max_len)[\n",
        "    None, ...]\n",
        "  pad_len = max(0, max_len - input_len)\n",
        "  padded_input_mask= jnp.pad(input_mask, [(0, 0), (0, pad_len)])\n",
        "  attention_mask = attention_mask * (input_mask[..., None] \n",
        "                                     * padded_input_mask[..., None, :])\n",
        "  return positions, attention_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "57nMYQ4HESaN"
      },
      "outputs": [],
      "source": [
        "# Load parameters\n",
        "with jax.default_device(cpu_device):\n",
        "  params_host = params_lib.load_and_format_params(ckpt_path)\n",
        "config = transformer_lib.TransformerConfig.from_params(\n",
        "    params_host,\n",
        "    cache_size=128  # Number of time steps in the transformer's cache\n",
        ")\n",
        "#config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=32)\n",
        "transformer = transformer_lib.Transformer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_param = lambda x: isinstance(x, nn.LogicallyPartitioned)\n",
        "\n",
        "def init_params(batch_size: int):\n",
        "  input_len = 1 # or 1 or 7, this dimension doesn't matter in initialization\n",
        "  random_key = random.key(0)\n",
        "  input_sequence = jnp.zeros((batch_size, input_len), dtype=jnp.int32)\n",
        "  positions = jnp.broadcast_to(\n",
        "    jnp.arange(input_sequence.shape[-1]).astype(jnp.int32), \n",
        "    input_sequence.shape)\n",
        "  attention_mask = jnp.ones((batch_size, input_len, config.max_cache_length), \n",
        "                            dtype=jnp.bool)\n",
        "  cache = config.init_cache(batch_size, jnp.float32, logically_partitioned=True)\n",
        "  cache_value = jax.tree.map(lambda x: x.value if is_param(x) else x, cache, \n",
        "                             is_leaf=is_param)\n",
        "  params = transformer.init(random_key, input_sequence, positions, \n",
        "                            cache_value, attention_mask)\n",
        "  return (params, cache)\n",
        "  \n",
        "BATCH_SIZE = 2\n",
        "params_struct, cache_struct = jax.eval_shape(lambda: init_params(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ffw', 'features', 'kv_heads', 'head_dim', 'batch', 'vocab', 'sequence', None, 'q_heads'}\n"
          ]
        }
      ],
      "source": [
        "axis_names = jax.tree.reduce(lambda x, y: x | set(y.names),\n",
        "        (params_struct, cache_struct), initializer=set(), is_leaf=is_param)\n",
        "print(axis_names)\n",
        "fsdp_rules = {\n",
        "  None: None, \n",
        "  \"batch\": None,\n",
        "  \"sequence\": None,\n",
        "  \"vocab\": None, \n",
        "  \"features\": \"x\",  # or 'd_model'\n",
        "  \"q_heads\": None, \n",
        "  \"kv_heads\": None, \n",
        "  \"head_dim\": None, \n",
        "  \"ffw\": None\n",
        "}\n",
        "assert all(k in fsdp_rules for k in axis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh = Mesh(devices, (\"x\",))\n",
        "#mesh = Mesh(devices[:1], (\"x\",))\n",
        "rules = fsdp_rules\n",
        "params_sharding, cache_sharding = jax.tree.map(\n",
        "  lambda x: NamedSharding(mesh, P(*[rules[name] for name in x.names])),\n",
        "  (params_struct, cache_struct), is_leaf=is_param)\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(0,), \n",
        "                   out_shardings=(params_sharding, cache_sharding))\n",
        "def unpack_params(batch_size: int) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "  params_cache = init_params(batch_size)\n",
        "  # unpack the parameters from the nn.LogicallyPartitioned wrapper\n",
        "  return jax.tree.map(lambda x: x.value, params_cache, is_leaf=is_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#params, cache = unpack_params(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-01 02:03:07.031586: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
          ]
        }
      ],
      "source": [
        "with jax.default_device(cpu_device):\n",
        "  params_host = params_lib.load_and_format_params(ckpt_path)\n",
        "params = jax.tree.map(lambda x, y: jax.device_put(x.astype(jnp.float32), y), \n",
        "                      {\"params\": params_host[\"transformer\"]}, params_sharding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────┬───────┬───────┬───────┐\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│ GPU 0 │ GPU 1 │ GPU 2 │ GPU 3 │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "└───────┴───────┴───────┴───────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┌───────┬───────┬───────┬───────┐\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│ GPU 0 │ GPU 1 │ GPU 2 │ GPU 3 │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "└───────┴───────┴───────┴───────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "jax.debug.visualize_array_sharding(params[\"params\"][\"layer_0\"][\"mlp\"][\"linear\"])\n",
        "#jax.debug.visualize_array_sharding(params_cache[\"params\"][\"params\"][\"layer_0\"][\"scale\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=(\"config\",), \n",
        "                   in_shardings=(params_sharding,  None))\n",
        "def prefill(params: dict[str, Any], input: jax.Array, \n",
        "            config: transformer_lib.TransformerConfig):\n",
        "  assert input.ndim == 2\n",
        "  batch_size, input_len = input.shape\n",
        "  max_len: int = config.max_cache_length\n",
        "  positions, attention_mask = construct_positions_and_attn_mask(input, max_len)\n",
        "  cache = jax.lax.with_sharding_constraint(\n",
        "    config.init_cache(batch_size, dtype=jnp.float32), cache_sharding)\n",
        "  logits, cache = transformer.apply(params, input, positions, cache, \n",
        "                                    attention_mask)\n",
        "  return logits, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def right_align_sequences(inputs: list[str]) -> jax.Array:\n",
        "  encoded = [vocab.Encode(input) for input in inputs]\n",
        "  max_len = max([len(x) for x in encoded])\n",
        "  return jnp.array([[vocab.pad_id()] * max(0, max_len - len(x)) \n",
        "                    + [vocab.bos_id()] + x for x in encoded])\n",
        "  \n",
        "batch_input = right_align_sequences([\"hello, how are you?\", \"The weather today is\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_input =\n",
            "[[     2  17534 235269   1368    708    692 235336]\n",
            " [     0      0      2    651   8957   3646    603]]\n",
            "positions =\n",
            "[[0 1 2 3 4 5 6]\n",
            " [0 0 0 1 2 3 4]]\n",
            "attn_mask =\n",
            "[[[1 0 0 0 0 0 0 0]\n",
            "  [1 1 0 0 0 0 0 0]\n",
            "  [1 1 1 0 0 0 0 0]\n",
            "  [1 1 1 1 0 0 0 0]\n",
            "  [1 1 1 1 1 0 0 0]\n",
            "  [1 1 1 1 1 1 0 0]\n",
            "  [1 1 1 1 1 1 1 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 0 0 0 0 0]\n",
            "  [0 0 1 1 0 0 0 0]\n",
            "  [0 0 1 1 1 0 0 0]\n",
            "  [0 0 1 1 1 1 0 0]\n",
            "  [0 0 1 1 1 1 1 0]]]\n"
          ]
        }
      ],
      "source": [
        "batch_input\n",
        "input = jnp.asarray(batch_input, dtype=jnp.int32)\n",
        "positions, attn_mask = construct_positions_and_attn_mask(input, 8)\n",
        "\n",
        "print(f\"batch_input =\\n{batch_input}\")\n",
        "print(f\"positions =\\n{positions}\")\n",
        "print(f\"attn_mask =\\n{attn_mask * 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=(\"config\", \"max_len\"), \n",
        "                   in_shardings=(params_sharding, cache_sharding, None))\n",
        "def decode(params: dict[str, Any], cache: dict[str, Any], input: jax.Array, \n",
        "            config: transformer_lib.TransformerConfig, max_len: int = -1):\n",
        "  if max_len < 0:\n",
        "    max_len = config.max_cache_length\n",
        "  assert max_len <= config.max_cache_length\n",
        "\n",
        "  idx = input.shape[-1]\n",
        "  tokens = jnp.ones(input.shape[:-1] + (max_len,), \n",
        "                    dtype=jnp.int32) * (PAD_ID + 1)\n",
        "  tokens = tokens.at[..., :idx].set(input)\n",
        "\n",
        "  positions, attn_mask = construct_positions_and_attn_mask(\n",
        "    tokens, max_len=config.max_cache_length)\n",
        "\n",
        "  def _decode_step(i, carry):\n",
        "    decode_tokens, tokens, cache = carry\n",
        "    #decode_tokens = jax.lax.dynamic_slice_in_dim(tokens, i - 1, 1, axis=-1)\n",
        "    curr_positions = jax.lax.dynamic_slice_in_dim(positions, i - 1, 1, axis=-1)\n",
        "    curr_attn_mask = jax.lax.dynamic_slice_in_dim(attn_mask, i - 1, 1, axis=-2)\n",
        "    # jax.debug.print(\"i = {}\", i)\n",
        "    # jax.debug.print(\"positions = {}\", positions[:, :32])\n",
        "    # jax.debug.print(\"attn_mask = {}\", curr_attn_mask[:, :, :32] * 1)\n",
        "    # jax.debug.print(\"decode_tokens = {}\", decode_tokens)\n",
        "    logits, cache = transformer.apply(params, decode_tokens, curr_positions, \n",
        "                                      cache, curr_attn_mask)                                     \n",
        "    next_tokens = jnp.argmax(logits, -1)[..., 0]\n",
        "    tokens = jax.lax.dynamic_update_index_in_dim(tokens, next_tokens, i, \n",
        "                                                 axis=-1)\n",
        "    # jax.debug.print(\"iterate = {}\", i)\n",
        "    # jax.debug.print(\"next_tokens = {}\", next_tokens)\n",
        "    # jax.debug.print(\"tokens now = {}\", tokens)\n",
        "    return next_tokens[..., None], tokens, cache\n",
        "  \n",
        "  #jax.debug.print(\"tokens initially = {}\", tokens)\n",
        "  \n",
        "  decode_tokens = tokens[:, idx-1:idx]\n",
        "  decode_tokens, new_tokens, new_cache= jax.lax.fori_loop(\n",
        "    idx, max_len, _decode_step, (decode_tokens, tokens, cache))\n",
        "  return new_tokens, new_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[     2  17534 235269   1368    708    692 235336]\n",
            " [     0      0      2    651   8957   3646    603]]\n",
            "[7 7]\n"
          ]
        }
      ],
      "source": [
        "#batch_input = right_align_sequences([\"\\n# Python program for implementation of Bubble Sort\\n\\ndef bubbleSort(arr):\"])\n",
        "#batch_input = right_align_sequences([\"hello\"])\n",
        "#batch_input = right_align_sequences([\"Tell me how you are: I'm\"])\n",
        "logits, prefilled_cache = prefill(params, batch_input, config)\n",
        "print(batch_input)\n",
        "print(prefilled_cache[\"layer_0\"][\"end_index\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "positions, attn_mask = construct_positions_and_attn_mask(batch_input, \n",
        "                                                         config.max_cache_length)\n",
        "cache = config.init_cache(batch_input.shape[0], dtype=jnp.float32)\n",
        "logits, _ = transformer.apply(params, batch_input, positions, \n",
        "                                      cache, attn_mask)                                     \n",
        "\n",
        "logits, prefilled_cache = prefill(params, batch_input, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_logit = jnp.argmax(logits[0, -1, :])\n",
        "vocab.Decode(new_logit.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[-4.18090820e-03,  5.70678711e-03,  4.11987305e-03, ...,\n",
              "        -7.01904297e-03,  1.16577148e-02, -7.44628906e-03],\n",
              "       [-1.94091797e-02,  3.35693359e-03,  6.77490234e-03, ...,\n",
              "        -9.07897949e-04,  1.08032227e-02, -9.46044922e-03],\n",
              "       [ 1.28936768e-03, -7.62939453e-03,  6.14166260e-04, ...,\n",
              "        -1.68609619e-03,  1.84774399e-05,  1.06811523e-02],\n",
              "       ...,\n",
              "       [-4.15802002e-04, -5.49316406e-03,  1.61132812e-02, ...,\n",
              "        -1.19018555e-02,  9.39941406e-03, -3.73840332e-04],\n",
              "       [-5.49316406e-03,  2.05993652e-03,  2.57873535e-03, ...,\n",
              "        -1.23291016e-02,  6.74438477e-03,  8.58306885e-05],\n",
              "       [ 8.11767578e-03,  6.34765625e-03, -1.46031380e-05, ...,\n",
              "         3.78417969e-03,  4.79125977e-03, -3.16619873e-04]],      dtype=float32)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params[\"params\"][\"layer_0\"][\"mlp\"][\"linear\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prefilled_cache[\"layer_0\"][\"k\"][0, :, 0, 0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits, prefilled_cache = prefill(params, batch_input, config)\n",
        "new_tokens, new_cache = decode(params, prefilled_cache, batch_input, config, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[     2,  17534, 235269,   1368,    708,    692, 235336,    109,\n",
              "        235285, 235303, 235262,   3900,   1009,  21426,    577,   4771,\n",
              "           970,  16572,    578,   8691,   2962, 235265,    590, 235303,\n",
              "        235262,   7965,   4786,   1426,    712,   2166, 235269,    901,\n",
              "           590, 235303, 235262,   2593,   3648,    604,    888,    578,\n",
              "         17305,   2652,    577,    749, 235265,    109,   1841,    708,\n",
              "          1009,  15513,    604,   2652,    577,    749,    674,   1134,\n",
              "           614,   2245,    578,  30509, 235336,    109, 235285, 235303,\n",
              "        235258,   2182,    577,   4675,    861,   9398,    611,    736,\n",
              "         11381, 235265,   5651,   2375,   2223,    577,   4638,   1089,\n",
              "          5793,    692,    791, 235269,    793,   4391,   1368,  14565,\n",
              "           984,   1249,   3307, 235265,    109,   4127,    692,    604,\n",
              "           861,   1069,    578,  12924, 235265,    109,    688,   4858,\n",
              "           708,   1009,  15513,    604,   2652,    577,    749,    674,\n",
              "          1134,    614,   2245,    578,  30509,  66058,    109, 235287,\n",
              "          5231,  11597,    476,    888,   7888,    689,   5640,  66058],\n",
              "       [     0,      0,      2,    651,   8957,   3646,    603,   4964,\n",
              "        235269,    675,    476,   3110,   8203,    578,   8056,  17187,\n",
              "        235265,   1165,    603,    476,   4434,   1744,    604,    476,\n",
              "         43834,  43834,    575,    573,   7441,   7441, 235265,    109,\n",
              "          1841,    603,    573,   8957,  23400,    604,  14610, 235336,\n",
              "           109, 235285,   1144,  14321,    577,   3658,   8957,  58330,\n",
              "           604,    573,   3936, 235269,    685,    590,    749,    780,\n",
              "           791,   3684,    577,   1879, 235290,   1602,   1423, 235265,\n",
              "             1,      1,      1,    651,   4807,   1721,    780,   5512,\n",
              "          4341,   1105,    573,   8957,  23400,    604,  14610, 235269,\n",
              "           712,    590,   2952,   3448,    736,   2872,    774,    573,\n",
              "          4646,   4807, 235265,      1,      1,      1,      1,    651,\n",
              "          4807,   1721,    780,   5512,   4341,   1105,    573,   8957,\n",
              "         23400,    604,  14610, 235269,    712,    590,   2952,   3448,\n",
              "           736,   2872,    774,    573,   4646,   4807, 235265,      1,\n",
              "             1,      1,      1,      1,    651,   4807,   1721,    780]],      dtype=int32)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[     2,  17534, 235269,   1368,    708,    692, 235336],\n",
              "       [     0,      0,      2,    651,   8957,   3646,    603]],      dtype=int32)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 0: `hello, how are you?`\n",
            "Prompt 1: `The weather today is`\n",
            "################################################################################\n",
            "Response 0:\n",
            "```\n",
            "hello, how are you?\n",
            "\n",
            "I'm doing some exercises to improve my fitness and overall health. I'm feeling pretty good so far, but I'm always looking for new and exciting things to do.\n",
            "\n",
            "What are some suggestions for things to do that would be fun and engaging?\n",
            "\n",
            "I'd love to hear your thoughts on this topic. Please feel free to share any ideas you have, no matter how crazy they may seem.\n",
            "\n",
            "Thank you for your time and consideration.\n",
            "\n",
            "**Here are some suggestions for things to do that would be fun and engaging:**\n",
            "\n",
            "* **Try a new sport or activity:**\n",
            "```\n",
            "--------------------------------------------------------------------------------\n",
            "Response 1:\n",
            "```\n",
            "The weather today is beautiful, with a clear sky and warm temperatures. It is a perfect day for a picnic picnic in the park park.\n",
            "\n",
            "What is the weather forecast for tomorrow?\n",
            "\n",
            "I am unable to provide weather forecasts for the future, as I do not have access to real-time data.The context does not mention anything about the weather forecast for tomorrow, so I cannot answer this question from the provided context.The context does not mention anything about the weather forecast for tomorrow, so I cannot answer this question from the provided context.The context does not\n",
            "```\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(batch_input.shape[0]):\n",
        "  print(f\"Prompt {i}: `{vocab.Decode(batch_input[i, :].tolist())}`\")\n",
        "print(\"#\" * 80)\n",
        "for i in range(new_tokens.shape[0]):\n",
        "  print(f\"Response {i}:\\n```\\n{vocab.Decode(new_tokens[i, :].tolist())}\\n```\")\n",
        "  print(\"-\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "gemma",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
