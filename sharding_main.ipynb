{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "likVQiEEYS5X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['XLA_PYTHON_CLIENT_PREALLOCATE = false']\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import functools\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import random\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "from flax import linen as nn\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "import sentencepiece as spm\n",
        "import kagglehub\n",
        "# kagglehub.login() # you might need to log in\n",
        "\n",
        "cpu_device, devices = jax.devices(\"cpu\")[0], jax.devices(\"cuda\")\n",
        "pprint([f\"{k} = {v}\" for k, v in os.environ.items() if k.startswith(\"XLA\")])\n",
        "\n",
        "jax.config.update(\"jax_compilation_cache_dir\", \n",
        "                  str(Path(\"~/.cache/jax_compilation_cache\").expanduser()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/rdyro/.cache/kagglehub/models/google/gemma-2/flax/gemma2-2b-it/1\n"
          ]
        }
      ],
      "source": [
        "# variants v1 have gemma/Flax\n",
        "# variant = '2b-it' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "# weights_dir = kagglehub.model_download(f'google/gemma/Flax/{variant}')\n",
        "\n",
        "variant = \"gemma2-2b-it\"\n",
        "weights_dir = kagglehub.model_download(f\"google/gemma-2/flax/{variant}\")\n",
        "print(weights_dir)\n",
        "ckpt_path = os.path.join(weights_dir, variant)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.Load(vocab_path)\n",
        "PAD_ID = vocab.pad_id()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums=(0, 1))\n",
        "def casual_attention_mask(seq_len: int, max_seq_len: int) -> jax.Array:\n",
        "  return jnp.arange(seq_len)[..., None] >= jnp.arange(max_seq_len)[None, ...]\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def construct_positions_and_attn_mask(input: jax.Array, max_len: int, \n",
        "                                      pad_id: int = PAD_ID\n",
        "                                      ) -> tuple[jax.Array, jax.Array]:\n",
        "  assert input.ndim == 2 and input.shape[-1] <= max_len\n",
        "  input_len = input.shape[-1]\n",
        "  input = input.astype(jnp.int32)\n",
        "  input_mask = input != pad_id\n",
        "  # positions are zero-indexed, cumsum gives one-indexed values\n",
        "  positions = ((jnp.cumsum(input_mask, axis=-1, dtype=jnp.int32) - 1) \n",
        "               * input_mask)\n",
        "  attention_mask = casual_attention_mask(input_len, max_len)[\n",
        "    None, ...]\n",
        "  pad_len = max(0, max_len - input_len)\n",
        "  padded_input_mask= jnp.pad(input_mask, [(0, 0), (0, pad_len)])\n",
        "  attention_mask = attention_mask * (input_mask[..., None] \n",
        "                                     * padded_input_mask[..., None, :])\n",
        "  return positions, attention_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "57nMYQ4HESaN"
      },
      "outputs": [],
      "source": [
        "# with jax.default_device(cpu_device):\n",
        "#   params_host = params_lib.load_and_format_params(ckpt_path)\n",
        "# config = transformer_lib.TransformerConfig.from_params(\n",
        "#     params_host,\n",
        "#     cache_size=128  # Number of time steps in the transformer's cache\n",
        "# )\n",
        "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=128)\n",
        "transformer = transformer_lib.Transformer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_param = lambda x: isinstance(x, nn.LogicallyPartitioned)\n",
        "\n",
        "def init_params(batch_size: int, dtype=jnp.bfloat16):\n",
        "  input_len = 1 # or 1 or 7, this dimension doesn't matter in initialization\n",
        "  random_key = random.key(0)\n",
        "  input_sequence = jnp.zeros((batch_size, input_len), dtype=jnp.int32)\n",
        "  positions, attn_mask = construct_positions_and_attn_mask(\n",
        "    input_sequence, config.max_cache_length)\n",
        "  cache = config.init_cache(batch_size, jnp.float32, logically_partitioned=True)\n",
        "  cache_value = jax.tree.map(lambda x: x.value if is_param(x) else x, cache, \n",
        "                             is_leaf=is_param)\n",
        "  params = transformer.init(random_key, input_sequence, positions, \n",
        "                            cache_value, attn_mask)\n",
        "  return (params, cache)\n",
        "  \n",
        "# we use jax.eval_shape to get just the shape of the parameters for sharding\n",
        "BATCH_SIZE = 1\n",
        "params_struct, cache_struct = jax.eval_shape(lambda: init_params(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logical axis names = {'vocab', 'features', 'ffw', 'sequence', 'q_heads', 'kv_heads', 'head_dim', 'batch', None}\n"
          ]
        }
      ],
      "source": [
        "axis_names = jax.tree.reduce(lambda x, y: x | set(y.names),\n",
        "        (params_struct, cache_struct), initializer=set(), is_leaf=is_param)\n",
        "print(f\"logical axis names = {axis_names}\")\n",
        "fsdp_rules = {\n",
        "  None: None, \n",
        "  \"batch\": None,\n",
        "  \"sequence\": None,\n",
        "  \"vocab\": None, \n",
        "  \"features\": \"x\",  # or 'd_model'\n",
        "  \"q_heads\": None, \n",
        "  \"kv_heads\": None, \n",
        "  \"head_dim\": None, \n",
        "  \"ffw\": None\n",
        "}\n",
        "assert all(k in fsdp_rules for k in axis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh = Mesh(devices, (\"x\",))\n",
        "rules = fsdp_rules\n",
        "params_sharding, cache_sharding = jax.tree.map(\n",
        "  lambda x: NamedSharding(mesh, P(*[rules[name] for name in x.names])),\n",
        "  (params_struct, cache_struct), is_leaf=is_param)\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(0, 1), \n",
        "                   out_shardings=(params_sharding, cache_sharding))\n",
        "def unpack_params(batch_size: int, dtype=jnp.bfloat16\n",
        "                  ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "  to_dtype = (lambda x: x.astype(dtype) \n",
        "              if jnp.issubdtype(x.dtype, jnp.floating) else x)\n",
        "  # the model is initialized in float32, we don't have much of a choice\n",
        "  # we could generate our own random weights from the model weight shapes\n",
        "  # but we want to use the initializers that the model authors used\n",
        "  params_cache = jax.tree.map(to_dtype, init_params(batch_size))\n",
        "  # unpack the parameters from the nn.LogicallyPartitioned wrapper\n",
        "  return jax.tree.map(lambda x: x.value, params_cache, is_leaf=is_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOAD_PARAMETERS = True\n",
        "DTYPE = jnp.bfloat16\n",
        "\n",
        "if LOAD_PARAMETERS:\n",
        "  with jax.default_device(cpu_device):\n",
        "    params_host = params_lib.load_and_format_params(ckpt_path)\n",
        "    params = jax.tree.map(lambda x, y: jax.device_put(x.astype(DTYPE), y), \n",
        "                          {\"params\": params_host[\"transformer\"]}, params_sharding)\n",
        "else:                \n",
        "  params, cache = unpack_params(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚ GPU 0 â”‚ GPU 1 â”‚ GPU 2 â”‚ GPU 3 â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚ GPU 0 â”‚ GPU 1 â”‚ GPU 2 â”‚ GPU 3 â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualize the sharding on an example layer\n",
        "jax.debug.visualize_array_sharding(params[\"params\"][\"layer_0\"][\"mlp\"][\"linear\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=(\"config\",), \n",
        "                   in_shardings=(params_sharding,  None))\n",
        "def prefill(params: dict[str, Any], input: jax.Array, \n",
        "            config: transformer_lib.TransformerConfig):\n",
        "  assert input.ndim == 2\n",
        "  batch_size, input_len = input.shape\n",
        "  max_len: int = config.max_cache_length\n",
        "  positions, attention_mask = construct_positions_and_attn_mask(input, max_len)\n",
        "  dtype = jax.tree.flatten(params)[0][0].dtype\n",
        "  cache = jax.lax.with_sharding_constraint(\n",
        "    config.init_cache(batch_size, dtype=dtype), cache_sharding)\n",
        "  logits, cache = transformer.apply(params, input, positions, cache, \n",
        "                                    attention_mask)\n",
        "  return logits, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-01 02:35:37.510353: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
          ]
        }
      ],
      "source": [
        "def right_align_sequences(inputs: list[str]) -> jax.Array:\n",
        "  encoded = [vocab.Encode(input) for input in inputs]\n",
        "  max_len = max([len(x) for x in encoded])\n",
        "  \n",
        "  # NEED TO add bos at the beginning or the model will give very bad results\n",
        "  return jnp.array([[vocab.pad_id()] * max(0, max_len - len(x)) \n",
        "                    + [vocab.bos_id()] + x for x in encoded])\n",
        "  \n",
        "batch_input = right_align_sequences([\"hello, how are you?\", \"The weather today is\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_input =\n",
            "[[     2  17534 235269   1368    708    692 235336]\n",
            " [     0      0      2    651   8957   3646    603]]\n",
            "positions =\n",
            "[[0 1 2 3 4 5 6]\n",
            " [0 0 0 1 2 3 4]]\n",
            "attn_mask =\n",
            "[[[1 0 0 0 0 0 0 0]\n",
            "  [1 1 0 0 0 0 0 0]\n",
            "  [1 1 1 0 0 0 0 0]\n",
            "  [1 1 1 1 0 0 0 0]\n",
            "  [1 1 1 1 1 0 0 0]\n",
            "  [1 1 1 1 1 1 0 0]\n",
            "  [1 1 1 1 1 1 1 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 0 0 0 0 0]\n",
            "  [0 0 1 1 0 0 0 0]\n",
            "  [0 0 1 1 1 0 0 0]\n",
            "  [0 0 1 1 1 1 0 0]\n",
            "  [0 0 1 1 1 1 1 0]]]\n"
          ]
        }
      ],
      "source": [
        "# let's explore how right-aligned sequences have their mask generated\n",
        "input = jnp.asarray(batch_input, dtype=jnp.int32)\n",
        "positions, attn_mask = construct_positions_and_attn_mask(input, 8)\n",
        "\n",
        "print(f\"batch_input =\\n{batch_input}\")\n",
        "print(f\"positions =\\n{positions}\")\n",
        "print(f\"attn_mask =\\n{attn_mask * 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=(\"config\", \"max_len\"), \n",
        "                   in_shardings=(params_sharding, cache_sharding, None))\n",
        "def decode(params: dict[str, Any], cache: dict[str, Any], input: jax.Array, \n",
        "            config: transformer_lib.TransformerConfig, max_len: int = -1):\n",
        "  if max_len < 0:\n",
        "    max_len = config.max_cache_length\n",
        "  assert max_len <= config.max_cache_length\n",
        "\n",
        "  idx = input.shape[-1]\n",
        "  tokens = jnp.ones(input.shape[:-1] + (max_len,), \n",
        "                    dtype=jnp.int32) * (PAD_ID + 1)\n",
        "  tokens = tokens.at[..., :idx].set(input)\n",
        "\n",
        "  positions, attn_mask = construct_positions_and_attn_mask(\n",
        "    tokens, max_len=config.max_cache_length)\n",
        "\n",
        "  def _decode_step(i, carry):\n",
        "    decode_tokens, tokens, cache = carry\n",
        "    #decode_tokens = jax.lax.dynamic_slice_in_dim(tokens, i - 1, 1, axis=-1)\n",
        "    curr_positions = jax.lax.dynamic_slice_in_dim(positions, i - 1, 1, axis=-1)\n",
        "    curr_attn_mask = jax.lax.dynamic_slice_in_dim(attn_mask, i - 1, 1, axis=-2)\n",
        "    # jax.debug.print(\"i = {}\", i)\n",
        "    # jax.debug.print(\"positions = {}\", positions[:, :32])\n",
        "    # jax.debug.print(\"attn_mask = {}\", curr_attn_mask[:, :, :32] * 1)\n",
        "    # jax.debug.print(\"decode_tokens = {}\", decode_tokens)\n",
        "    logits, cache = transformer.apply(params, decode_tokens, curr_positions, \n",
        "                                      cache, curr_attn_mask)                                     \n",
        "    next_tokens = jnp.argmax(logits, -1)[..., 0]\n",
        "    tokens = jax.lax.dynamic_update_index_in_dim(tokens, next_tokens, i, \n",
        "                                                 axis=-1)\n",
        "    # jax.debug.print(\"iterate = {}\", i)\n",
        "    # jax.debug.print(\"next_tokens = {}\", next_tokens)\n",
        "    # jax.debug.print(\"tokens now = {}\", tokens)\n",
        "    return next_tokens[..., None], tokens, cache\n",
        "  \n",
        "  #jax.debug.print(\"tokens initially = {}\", tokens)\n",
        "  \n",
        "  decode_tokens = tokens[:, idx-1:idx]\n",
        "  decode_tokens, new_tokens, new_cache= jax.lax.fori_loop(\n",
        "    idx, max_len, _decode_step, (decode_tokens, tokens, cache))\n",
        "  return new_tokens, new_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#batch_input = right_align_sequences([\"\\n# Python program for implementation of Bubble Sort\\n\\ndef bubbleSort(arr):\"])\n",
        "#batch_input = right_align_sequences([\"hello\"])\n",
        "#batch_input = right_align_sequences([\"Tell me how you are: I'm\"])\n",
        "logits, prefilled_cache = prefill(params, batch_input, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits, prefilled_cache = prefill(params, batch_input, config)\n",
        "new_tokens, new_cache = decode(params, prefilled_cache, batch_input, config, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 0: `hello, how are you?`\n",
            "Prompt 1: `The weather today is`\n",
            "################################################################################\n",
            "Response 0:\n",
            "```\n",
            "hello, how are you?\n",
            "\n",
            "I am doing well, thank you for asking. ğŸ˜Š  How are you doing today? \n",
            "<end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn>\n",
            "<end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn>\n",
            "```\n",
            "--------------------------------------------------------------------------------\n",
            "Response 1:\n",
            "```\n",
            "The weather today is a bit of a mixed bag. It's not quite cold enough for a winter coat, but it's definitely not warm enough for a t-shirt. It's a bit of a grey day, with some patches of sunshine peeking through. \n",
            "\n",
            "I'm feeling a bit of a mixed bag myself today. I'm excited about the upcoming weekend, but I'm also feeling a bit overwhelmed with all the things I need to get done. \n",
            "\n",
            "What's your weather like today? And how are you feeling today? \n",
            "<end_of_turn>\n",
            "```\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(batch_input.shape[0]):\n",
        "  print(f\"Prompt {i}: `{vocab.Decode(batch_input[i, :].tolist())}`\")\n",
        "print(\"#\" * 80)\n",
        "for i in range(new_tokens.shape[0]):\n",
        "  print(f\"Response {i}:\\n```\\n{vocab.Decode(new_tokens[i, :].tolist())}\\n```\")\n",
        "  print(\"-\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "gemma",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
