{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "likVQiEEYS5X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "['XLA_PYTHON_CLIENT_PREALLOCATE = false']\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import functools\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "import math\n",
        "from typing import Any\n",
        "\n",
        "# to observe the actual memory getting somewhat conservatively allocated\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import random\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "from flax import linen as nn\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "import sentencepiece as spm\n",
        "import kagglehub\n",
        "# kagglehub.login() # you might need to log in\n",
        "\n",
        "cpu_device, devices = jax.devices(\"cpu\")[0], jax.devices(\"cuda\")\n",
        "pprint([f\"{k} = {v}\" for k, v in os.environ.items() if k.startswith(\"XLA\")])\n",
        "\n",
        "jax.config.update(\"jax_compilation_cache_dir\", \n",
        "                  str(Path(\"~/.cache/jax_compilation_cache\").expanduser()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/rdyro/.cache/kagglehub/models/google/gemma-2/flax/gemma2-2b-it/1\n"
          ]
        }
      ],
      "source": [
        "# variants v1 have gemma/Flax\n",
        "# variant = '2b-it' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "# weights_dir = kagglehub.model_download(f'google/gemma/Flax/{variant}')\n",
        "\n",
        "variant = \"gemma2-2b-it\"\n",
        "weights_dir = kagglehub.model_download(f\"google/gemma-2/flax/{variant}\")\n",
        "print(weights_dir)\n",
        "ckpt_path = os.path.join(weights_dir, variant)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.Load(vocab_path)\n",
        "PAD_ID = vocab.pad_id()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums=(0, 1))\n",
        "def casual_attention_mask(seq_len: int, max_seq_len: int) -> jax.Array:\n",
        "  return jnp.arange(seq_len)[..., None] >= jnp.arange(max_seq_len)[None, ...]\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(1, 2))\n",
        "def construct_positions_and_attn_mask(input: jax.Array, max_len: int, \n",
        "                                      pad_id: int = PAD_ID\n",
        "                                      ) -> tuple[jax.Array, jax.Array]:\n",
        "  assert input.ndim == 2 and input.shape[-1] <= max_len\n",
        "  input_len = input.shape[-1]\n",
        "  input = input.astype(jnp.int32)\n",
        "  input_mask = input != pad_id\n",
        "  # positions are zero-indexed, cumsum gives one-indexed values\n",
        "  positions = ((jnp.cumsum(input_mask, axis=-1, dtype=jnp.int32) - 1) \n",
        "               * input_mask)\n",
        "  attention_mask = casual_attention_mask(input_len, max_len)[\n",
        "    None, ...]\n",
        "  pad_len = max(0, max_len - input_len)\n",
        "  padded_input_mask= jnp.pad(input_mask, [(0, 0), (0, pad_len)])\n",
        "  attention_mask = attention_mask * (input_mask[..., None] \n",
        "                                     * padded_input_mask[..., None, :])\n",
        "  return positions, attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "57nMYQ4HESaN"
      },
      "outputs": [],
      "source": [
        "# with jax.default_device(cpu_device):\n",
        "#   params_host = params_lib.load_and_format_params(ckpt_path)\n",
        "# config = transformer_lib.TransformerConfig.from_params(\n",
        "#     params_host,\n",
        "#     cache_size=128  # Number of time steps in the transformer's cache\n",
        "# )\n",
        "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=128)\n",
        "transformer = transformer_lib.Transformer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_param = lambda x: isinstance(x, nn.LogicallyPartitioned)\n",
        "\n",
        "def init_params(batch_size: int, dtype=jnp.bfloat16):\n",
        "  input_len = 1 # or 1 or 7, this dimension doesn't matter in initialization\n",
        "  random_key = random.key(0)\n",
        "  input_sequence = jnp.zeros((batch_size, input_len), dtype=jnp.int32)\n",
        "  positions, attn_mask = construct_positions_and_attn_mask(\n",
        "    input_sequence, config.max_cache_length)\n",
        "  cache = config.init_cache(batch_size, jnp.float32, logically_partitioned=True)\n",
        "  cache_value = jax.tree.map(lambda x: x.value if is_param(x) else x, cache, \n",
        "                             is_leaf=is_param)\n",
        "  params = transformer.init(random_key, input_sequence, positions, \n",
        "                            cache_value, attn_mask)\n",
        "  return (params, cache)\n",
        "  \n",
        "# we use jax.eval_shape to get just the shape of the parameters for sharding\n",
        "BATCH_SIZE = 1\n",
        "params_struct, cache_struct = jax.eval_shape(lambda: init_params(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logical axis names = {'vocab', 'batch', 'head_dim', 'ffw', 'features', 'kv_heads', 'q_heads', 'sequence', None}\n"
          ]
        }
      ],
      "source": [
        "axis_names = jax.tree.reduce(lambda x, y: x | set(y.names),\n",
        "        (params_struct, cache_struct), initializer=set(), is_leaf=is_param)\n",
        "print(f\"logical axis names = {axis_names}\")\n",
        "model_parallel_rules = {\n",
        "  None: None, \n",
        "  \"batch\": None,\n",
        "  \"sequence\": None,\n",
        "  \"vocab\": None, \n",
        "  \"features\": \"x\",  # or 'd_model'\n",
        "  \"q_heads\": None, \n",
        "  \"kv_heads\": None, \n",
        "  \"head_dim\": None, \n",
        "  \"ffw\": None\n",
        "}\n",
        "assert all(k in model_parallel_rules for k in axis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh = Mesh(devices, (\"x\",))\n",
        "rules = model_parallel_rules\n",
        "params_sharding, cache_sharding = jax.tree.map(\n",
        "  lambda x: NamedSharding(mesh, P(*[rules[name] for name in x.names])),\n",
        "  (params_struct, cache_struct), is_leaf=is_param)\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(0, 1), \n",
        "                   out_shardings=(params_sharding, cache_sharding))\n",
        "def unpack_params(batch_size: int, dtype=jnp.bfloat16\n",
        "                  ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "  to_dtype = (lambda x: x.astype(dtype) \n",
        "              if jnp.issubdtype(x.dtype, jnp.floating) else x)\n",
        "  # the model is initialized in float32, we don't have much of a choice\n",
        "  # we could generate our own random weights from the model weight shapes\n",
        "  # but we want to use the initializers that the model authors used\n",
        "  params_cache = jax.tree.map(to_dtype, init_params(batch_size))\n",
        "  # unpack the parameters from the nn.LogicallyPartitioned wrapper\n",
        "  return jax.tree.map(lambda x: x.value, params_cache, is_leaf=is_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOAD_PARAMETERS = True\n",
        "DTYPE = jnp.bfloat16\n",
        "\n",
        "if LOAD_PARAMETERS:\n",
        "  with jax.default_device(cpu_device):\n",
        "    params_host = params_lib.load_and_format_params(ckpt_path)\n",
        "    params = jax.tree.map(lambda x, y: jax.device_put(x.astype(DTYPE), y), \n",
        "                          {\"params\": params_host[\"transformer\"]}, params_sharding)\n",
        "else:                \n",
        "  params, cache = unpack_params(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global model size in bytes:     4.8701 GB\n",
            "Per-host model size in bytes:   4.8701 GB\n",
            "Per-device model size in bytes: 1.2175 GB\n"
          ]
        }
      ],
      "source": [
        "def get_size(arr: jax.Array):\n",
        "  shard_shapes = [shard.data.shape for shard in arr.addressable_shards]\n",
        "  global_shape = arr.shape\n",
        "  per_host_local_size = sum(math.prod(local_shape) * arr.dtype.itemsize \n",
        "                            for local_shape in shard_shapes)\n",
        "  per_device_local_size = math.prod(shard_shapes[0]) * arr.dtype.itemsize\n",
        "  global_size = math.prod(global_shape) * arr.dtype.itemsize\n",
        "  return (global_size, per_host_local_size, per_device_local_size)\n",
        "  \n",
        "def reduce_size(size1, size2):\n",
        "  return tuple(x1 + x2 for (x1, x2) in zip(size1, size2))\n",
        "\n",
        "global_size, per_host_size, per_device_size = jax.tree.reduce(\n",
        "  reduce_size, jax.tree.map(get_size, params), initializer=(0, 0, 0), \n",
        "  is_leaf=lambda a: isinstance(a, tuple))\n",
        "print(f\"Global model size in bytes:     {global_size / (1024 ** 3):.4f} GB\")\n",
        "print(f\"Per-host model size in bytes:   {per_host_size / (1024 ** 3):.4f} GB\")\n",
        "print(f\"Per-device model size in bytes: {per_device_size / (1024 ** 3):.4f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> â”‚ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> â”‚ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> â”‚ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚ GPU \u001b[1;36m0\u001b[0m â”‚ GPU \u001b[1;36m1\u001b[0m â”‚ GPU \u001b[1;36m2\u001b[0m â”‚ GPU \u001b[1;36m3\u001b[0m â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â”‚       â”‚       â”‚       â”‚       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualize the sharding on an example layer\n",
        "jax.debug.visualize_array_sharding(params[\"params\"][\"layer_0\"][\"mlp\"][\"linear\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=(\"config\",), \n",
        "                   in_shardings=(params_sharding,  None))\n",
        "def prefill(params: dict[str, Any], input: jax.Array, \n",
        "            config: transformer_lib.TransformerConfig):\n",
        "  assert input.ndim == 2\n",
        "  batch_size, input_len = input.shape\n",
        "  max_len: int = config.max_cache_length\n",
        "  positions, attention_mask = construct_positions_and_attn_mask(input, max_len)\n",
        "  dtype = jax.tree.flatten(params)[0][0].dtype\n",
        "  cache = jax.lax.with_sharding_constraint(\n",
        "    config.init_cache(batch_size, dtype=dtype), cache_sharding)\n",
        "  logits, cache = transformer.apply(params, input, positions, cache, \n",
        "                                    attention_mask)\n",
        "  return logits, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_id(text: str):\n",
        "  tokens = vocab.Encode(text)\n",
        "  assert len(tokens) == 1 and isinstance(tokens[0], int)\n",
        "  return tokens[0]\n",
        "\n",
        "sot_id, eot_id = get_id(\"<start_of_turn>\"), get_id(\"<end_of_turn>\")\n",
        "bos_id, pad_id = vocab.bos_id(), PAD_ID\n",
        "user_id, model_id, newline_id = get_id(\"user\"), get_id(\"model\"), get_id(\"\\n\")\n",
        "\n",
        "def right_align_sequences(inputs: list[str]) -> jax.Array:\n",
        "  encoded = [vocab.Encode(input) for input in inputs]\n",
        "  max_len = max([len(x) for x in encoded])\n",
        "  \n",
        "  # NEED TO add bos at the beginning or the model will give very bad results\n",
        "  # we then also fill in the model chat template: [bos_id, ...]\n",
        "  return jnp.array([[pad_id] * max(0, max_len - len(x)) \n",
        "                    + [bos_id, sot_id, user_id, newline_id] \n",
        "                    + x + [newline_id, eot_id, sot_id, model_id, newline_id]\n",
        "                    for x in encoded])\n",
        "  \n",
        "batch_input = right_align_sequences([\"hello, how are you?\", \n",
        "                                     \"The weather today is\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_input =\n",
            "[[     2    106   1645    108  17534 235269   1368    708    692 235336\n",
            "     108    107    106   2516    108]\n",
            " [     0      0      2    106   1645    108    651   8957   3646    603\n",
            "     108    107    106   2516    108]]\n",
            "positions =\n",
            "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  0  0  1  2  3  4  5  6  7  8  9 10 11 12]]\n",
            "attn_mask =\n",
            "[[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
            "  [0 0 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
            "  [0 0 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
            "  [0 0 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
            "  [0 0 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
            "  [0 0 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
            "  [0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "  [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]]]\n"
          ]
        }
      ],
      "source": [
        "# let's explore how right-aligned sequences have their mask generated\n",
        "input = jnp.asarray(batch_input, dtype=jnp.int32)\n",
        "positions, attn_mask = construct_positions_and_attn_mask(input, input.shape[-1])\n",
        "\n",
        "print(f\"batch_input =\\n{batch_input}\")\n",
        "print(f\"positions =\\n{positions}\")\n",
        "print(f\"attn_mask =\\n{attn_mask * 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=(\"config\", \"max_len\"), \n",
        "                   in_shardings=(params_sharding, cache_sharding, None))\n",
        "def decode(params: dict[str, Any], cache: dict[str, Any], input: jax.Array, \n",
        "            config: transformer_lib.TransformerConfig, max_len: int = -1):\n",
        "  if max_len < 0:\n",
        "    max_len = config.max_cache_length\n",
        "  assert max_len <= config.max_cache_length\n",
        "\n",
        "  idx = input.shape[-1]\n",
        "  tokens = jnp.ones(input.shape[:-1] + (max_len,), \n",
        "                    dtype=jnp.int32) * (PAD_ID + 1)\n",
        "  tokens = tokens.at[..., :idx].set(input)\n",
        "\n",
        "  # construct a position and attention mask for autoregressive decoding\n",
        "  positions, attn_mask = construct_positions_and_attn_mask(\n",
        "    tokens, max_len=config.max_cache_length)\n",
        "\n",
        "  # a decode step, carry = (current_tokens, all_tokens, autoregressive_cache)\n",
        "  def _decode_step(carry, i):\n",
        "    tokens, cache = carry\n",
        "    last_tokens = tokens[..., i-1][..., None]\n",
        "    curr_positions = positions[..., i-1][..., None]\n",
        "    curr_attn_mask = attn_mask[..., i-1, :][..., None, :]\n",
        "    logits, cache = transformer.apply(params, last_tokens, curr_positions, \n",
        "                                      cache, curr_attn_mask)                                     \n",
        "    next_tokens = jnp.argmax(logits, -1)[..., 0]\n",
        "    tokens = tokens.at[..., i].set(next_tokens)\n",
        "    carry = (tokens, cache)\n",
        "    return carry, 0\n",
        "  \n",
        "  autoreg_idxs = jnp.arange(idx, max_len)\n",
        "  # pseudocode for scan:\n",
        "  # carry_last, stacked_partial_results = scan(\n",
        "  #       lambda carry, i: function(carry, i), carry_init, \n",
        "  #       scanned_array = range(x, y))\n",
        "  (new_tokens, _), _ = jax.lax.scan(_decode_step, (tokens, cache), autoreg_idxs)\n",
        "  return new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits, prefilled_cache = prefill(params, batch_input, config)\n",
        "new_tokens = decode(params, prefilled_cache, batch_input, config, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 0: `<start_of_turn>user\n",
            "hello, how are you?\n",
            "<end_of_turn><start_of_turn>model\n",
            "`\n",
            "Prompt 1: `<start_of_turn>user\n",
            "The weather today is\n",
            "<end_of_turn><start_of_turn>model\n",
            "`\n",
            "################################################################################\n",
            "Response 0:\n",
            "```\n",
            "<start_of_turn>user\n",
            "hello, how are you?\n",
            "<end_of_turn><start_of_turn>model\n",
            "I'm doing well, thank you! ğŸ˜Š  \n",
            "\n",
            "How are you doing today?  \n",
            "<end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn>\n",
            "```\n",
            "--------------------------------------------------------------------------------\n",
            "Response 1:\n",
            "```\n",
            "<start_of_turn>user\n",
            "The weather today is\n",
            "<end_of_turn><start_of_turn>model\n",
            "Please tell me where you are located! I need a city or region to tell you the weather. ğŸ˜Š â˜€ï¸â˜€ï¸ğŸŒ§ï¸ğŸ’¨ğŸ’¨ \n",
            "<end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn>\n",
            "```\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(batch_input.shape[0]):\n",
        "  print(f\"Prompt {i}: `{vocab.Decode(batch_input[i, :].tolist())}`\")\n",
        "print(\"#\" * 80)\n",
        "for i in range(new_tokens.shape[0]):\n",
        "  print(f\"Response {i}:\\n```\\n{vocab.Decode(new_tokens[i, :].tolist())}\\n```\")\n",
        "  print(\"-\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "gemma",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
