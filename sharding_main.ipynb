{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "likVQiEEYS5X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "['XLA_PYTHON_CLIENT_PREALLOCATE = false']\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import functools\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import random\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "from flax import linen as nn\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "import sentencepiece as spm\n",
        "import kagglehub\n",
        "#kagglehub.login()\n",
        "\n",
        "cpu_device, devices = jax.devices(\"cpu\")[0], jax.devices(\"cuda\")\n",
        "pprint([f\"{k} = {v}\" for k, v in os.environ.items() if k.startswith(\"XLA\")])\n",
        "\n",
        "jax.config.update(\"jax_compilation_cache_dir\", \n",
        "                  str(Path(\"~/.cache/jax_compilation_cache\").expanduser()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums=(0, 1))\n",
        "def casual_attention_mask(seq_len: int, max_seq_len: int):\n",
        "  return jnp.arange(seq_len)[..., None] >= jnp.arange(max_seq_len)[None, ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O-sxcasvESaM"
      },
      "outputs": [],
      "source": [
        "variant = '2b-it' # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{variant}')\n",
        "ckpt_path = os.path.join(weights_dir, variant)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "57nMYQ4HESaN"
      },
      "outputs": [],
      "source": [
        "# Load parameters\n",
        "#params = params_lib.load_and_format_params(ckpt_path)\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.Load(vocab_path)\n",
        "#transformer_config=transformer_lib.TransformerConfig.from_params(\n",
        "#    params,\n",
        "#    cache_size=1024  # Number of time steps in the transformer's cache\n",
        "#)\n",
        "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=32)\n",
        "transformer = transformer_lib.Transformer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_param = lambda x: isinstance(x, nn.LogicallyPartitioned)\n",
        "\n",
        "def init_params(batch_size: int):\n",
        "  input_len = 1 # or 1 or 7, this dimension doesn't matter in initialization\n",
        "  random_key = random.key(0)\n",
        "  input_sequence = jnp.zeros((batch_size, input_len), dtype=jnp.int32)\n",
        "  positions = jnp.broadcast_to(\n",
        "    jnp.arange(input_sequence.shape[-1]).astype(jnp.int32), \n",
        "    input_sequence.shape)\n",
        "  attention_mask = jnp.ones((batch_size, input_len, config.max_cache_length), \n",
        "                            dtype=jnp.bool)\n",
        "  cache = config.init_cache(batch_size, jnp.float32)\n",
        "  cache_value = jax.tree.map(lambda x: x.value if is_param(x) else x, cache, \n",
        "                             is_leaf=is_param)\n",
        "  params = transformer.init(random_key, input_sequence, positions, \n",
        "                            cache_value, attention_mask)\n",
        "  return (params, cache)\n",
        "  \n",
        "BATCH_SIZE = 2\n",
        "params_struct, cache_struct = jax.eval_shape(lambda: init_params(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'kv_heads', 'features', 'ffw', 'head_dim', 'q_heads', 'batch', 'sequence', 'vocab', None}\n"
          ]
        }
      ],
      "source": [
        "axis_names = jax.tree.reduce(\n",
        "        lambda x, y: x | set(y.names if is_param(y) else []), \n",
        "        (params_struct, cache_struct), initializer=set(), is_leaf=is_param)\n",
        "print(axis_names)\n",
        "fsdp_rules = {\n",
        "  None: None, \n",
        "  \"batch\": None,\n",
        "  \"sequence\": None,\n",
        "  \"vocab\": None, \n",
        "  \"features\": \"x\",  # sholto calls this 'd_model'\n",
        "  \"q_heads\": None, \n",
        "  \"kv_heads\": None, \n",
        "  \"head_dim\": None, \n",
        "  \"ffw\": None\n",
        "}\n",
        "assert all(k in fsdp_rules for k in axis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh = Mesh(devices, (\"x\",))\n",
        "def logical_to_physical(rules, x):\n",
        "  return [rules[name] for name in x.names] if is_param(x) else ([None] * x.ndim)\n",
        "\n",
        "rules = fsdp_rules\n",
        "params_sharding, cache_sharding = jax.tree.map(\n",
        "  lambda x: NamedSharding(mesh, P(*logical_to_physical(rules, x))), \n",
        "  (params_struct, cache_struct), is_leaf=is_param)\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(0,), \n",
        "                   out_shardings=(params_sharding, cache_sharding))\n",
        "def unpack_params(batch_size: int) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "  params_cache = init_params(batch_size)\n",
        "  # unpack the parameters from the nn.LogicallyPartitioned wrapper\n",
        "  return jax.tree.map(lambda x: x.value if is_param(x) else x, params_cache, \n",
        "                      is_leaf=is_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "params, cache = unpack_params(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────┬───────┬───────┬───────┐\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│ GPU 0 │ GPU 1 │ GPU 2 │ GPU 3 │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "└───────┴───────┴───────┴───────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┌───────┬───────┬───────┬───────┐\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│ GPU 0 │ GPU 1 │ GPU 2 │ GPU 3 │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "│       │       │       │       │\n",
              "└───────┴───────┴───────┴───────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "jax.debug.visualize_array_sharding(params[\"params\"][\"layer_0\"][\"mlp\"][\"linear\"])\n",
        "#jax.debug.visualize_array_sharding(params_cache[\"params\"][\"params\"][\"layer_0\"][\"scale\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=(\"config\",), in_shardings=(params_sharding, cache_sharding, None))\n",
        "def prefill(params: dict[str, Any], cache: dict[str, Any], input: jax.Array, \n",
        "            config: transformer_lib.TransformerConfig):\n",
        "  assert input.ndim == 2\n",
        "  batch_size, input_len = input.shape\n",
        "  assert input_len <= config.max_cache_length\n",
        "  input = input.astype(jnp.int32)\n",
        "  input_mask = input != vocab.pad_id()\n",
        "\n",
        "  attention_mask = casual_attention_mask(input_len, config.max_cache_length)[\n",
        "    None, ...]\n",
        "  positions = jnp.broadcast_to(jnp.arange(input_len), input.shape)\n",
        "  positions = positions * input_mask\n",
        "  pad_len = max(0, config.max_cache_length - input_len)\n",
        "  padded_input_mask= jnp.pad(input_mask, [(0, 0), (0, pad_len)])\n",
        "  attention_mask = attention_mask * input_mask[..., None] * padded_input_mask[..., None, :]\n",
        "  logits, cache = transformer.apply(params, input, positions, cache, \n",
        "                                    attention_mask)\n",
        "  return logits, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stack_input(inputs: list[str]):\n",
        "  encoded = [vocab.Encode(input) for input in inputs]\n",
        "  max_len = max([len(x) for x in encoded])\n",
        "  return jnp.array([x + [vocab.pad_id()] * max(0, max_len - len(x)) for x in encoded])\n",
        "  \n",
        "batch_input = stack_input([\"hello, how are you?\", \"The weather today is\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[ 17534, 235269,   1368,    708,    692, 235336],\n",
              "       [   651,   8957,   3646,    603,      0,      0]], dtype=int32)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits, new_cache = prefill(params, cache, batch_input, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6 6]\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array([-0.74918693,  0.38351545,  0.7432269 ,  0.10356966,  0.17441484,\n",
              "        0.01317915,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ], dtype=float32)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(new_cache[\"layer_0\"][\"end_index\"])\n",
        "print()\n",
        "new_cache[\"layer_0\"][\"k\"][0, :, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────────────────────────────────────────────────────────────────────────┐\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                 GPU 0,1,2,3                                  │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "└──────────────────────────────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┌──────────────────────────────────────────────────────────────────────────────┐\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                 GPU 0,1,2,3                                  │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "│                                                                              │\n",
              "└──────────────────────────────────────────────────────────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "jax.debug.visualize_array_sharding(new_cache[\"layer_0\"][\"k\"][0, 0, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([6, 6], dtype=int32)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_cache[\"layer_0\"][\"end_index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_seq_len = config.max_cache_length\n",
        "input_sequence = jnp.array(vocab.Encode(\"Hello, what's your name\"), \n",
        "                           dtype=jnp.int32)[None, ...]\n",
        "                           \n",
        "assert input_sequence.ndim == 2\n",
        "cache = config.init_cache(1, dtype=jnp.float32)\n",
        "batch_size, input_len = input_sequence.shape\n",
        "input_mask = input_sequence != vocab.pad_id()\n",
        "positions = jnp.arange(input_len) * input_mask\n",
        "attention_mask = casual_attention_mask(input_len, max_seq_len)\n",
        "attention_mask = jnp.broadcast_to(attention_mask, \n",
        "                                  (batch_size, input_len, max_seq_len))\n",
        "\n",
        "attention_mask = attention_mask.at[:, :, :input_len].set(\n",
        "  attention_mask[:, :, :input_len] * input_mask[:, None, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "with jax.default_device(cpu_device):\n",
        "  #params = jax.eval_shape(transformer.init, random.key(0), \n",
        "  #                        input_sequence, positions, cache, attention_mask)\n",
        "  params = transformer.init(random.key(0), input_sequence, positions, cache, \n",
        "                            attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'params': {'embedder': {'input_embedding': ('vocab', 'd_model')},\n",
              "  'final_norm': {'scale': ('d_model',)},\n",
              "  'layer_0': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_1': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_10': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_11': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_12': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_13': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_14': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_15': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_16': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_17': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_18': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_19': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_2': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_20': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_21': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_22': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_23': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_24': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_25': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_3': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_4': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_5': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_6': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_7': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_8': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}},\n",
              "  'layer_9': {'attn': {'attn_vec_einsum': {'w': ('d_model',\n",
              "      'head_dim',\n",
              "      'features')},\n",
              "    'kv_einsum': {'w': (None, 'd_model', 'features', 'head_dim')},\n",
              "    'q_einsum': {'w': ('d_model', 'query_heads', 'head_dim')}},\n",
              "   'mlp': {'gating_einsum': (None, 'features', 'ffw'),\n",
              "    'linear': ('ffw', 'features')},\n",
              "   'post_attention_norm': {'scale': ('d_model',)},\n",
              "   'post_ffw_norm': {'scale': ('d_model',)},\n",
              "   'pre_attention_norm': {'scale': ('d_model',)},\n",
              "   'pre_ffw_norm': {'scale': ('d_model',)}}}}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jax.tree.map(lambda x: x.names, params, \n",
        "             is_leaf=lambda x: isinstance(x, nn.LogicallyPartitioned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{CudaDevice(id=0)}\n"
          ]
        }
      ],
      "source": [
        "cache = config.init_cache(1, jnp.float32)\n",
        "with jax.default_device(cpu_device):\n",
        "  y, cache = transformer.apply(params, input_sequence, positions, cache, \n",
        "                               attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaU-X3_jESaN"
      },
      "source": [
        "Finally, build a sampler on top of your model and your tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "cellView": "form",
        "id": "bdstASGrESaN"
      },
      "outputs": [],
      "source": [
        "# Create a sampler with the right param shapes.\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=transformer,\n",
        "    vocab=vocab,\n",
        "    #params=params['transformer'],\n",
        "    params=params,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1fLns-_ESaN"
      },
      "source": [
        "You're ready to start sampling ! This sampler uses just-in-time compilation, so changing the input shape triggers recompilation, which can slow things down. For the fastest and most efficient results, keep your batch size consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "cellView": "form",
        "id": "qA0BhNQvESaN"
      },
      "outputs": [
        {
          "ename": "ApplyScopeInvalidVariablesStructureError",
          "evalue": "Expect the `variables` (first argument) passed to apply() to be a dict with the structure {\"params\": ...}, but got a dict with an extra params layer, i.e.  {\"params\": {\"params\": ... } }. You should instead pass in your dict's [\"params\"]. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesStructureError)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApplyScopeInvalidVariablesStructureError\u001b[0m  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[46], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m input_batch \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Python program for implementation of Bubble Sort\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdef bubbleSort(arr):\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the planets of the solar system?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m   ]\n\u001b[0;32m----> 6\u001b[0m out_data \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_generation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of steps performed when generating\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_string, out_string \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_batch, out_data\u001b[38;5;241m.\u001b[39mtext):\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minput_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mout_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/gemma/gemma/sampler.py:334\u001b[0m, in \u001b[0;36mSampler.__call__\u001b[0;34m(self, input_strings, total_generation_steps, echo, return_logits, forbidden_tokens)\u001b[0m\n\u001b[1;32m    326\u001b[0m total_sampling_steps \u001b[38;5;241m=\u001b[39m max_input_length \u001b[38;5;241m+\u001b[39m total_generation_steps\n\u001b[1;32m    327\u001b[0m initial_sampling_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_sample_state(\n\u001b[1;32m    328\u001b[0m     all_input_ids,\n\u001b[1;32m    329\u001b[0m     include_logits\u001b[38;5;241m=\u001b[39mreturn_logits,\n\u001b[1;32m    330\u001b[0m     total_sampling_steps\u001b[38;5;241m=\u001b[39mtotal_sampling_steps,\n\u001b[1;32m    331\u001b[0m     forbidden_token_ids\u001b[38;5;241m=\u001b[39mforbidden_token_ids,\n\u001b[1;32m    332\u001b[0m )\n\u001b[0;32m--> 334\u001b[0m sampling_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiled_sample_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_sampling_state\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m masked_token_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_tokens_after_eos_ids(\n\u001b[1;32m    339\u001b[0m     sampling_state\u001b[38;5;241m.\u001b[39mtoken_buffer\n\u001b[1;32m    340\u001b[0m )\n\u001b[1;32m    342\u001b[0m out_tokens \u001b[38;5;241m=\u001b[39m []\n",
            "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
            "File \u001b[0;32m~/gemma/gemma/sampler.py:287\u001b[0m, in \u001b[0;36mSampler._sample_fn\u001b[0;34m(self, params, initial_sampling_state)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcond_fn\u001b[39m(sampler_state: _SamplingState):\n\u001b[1;32m    283\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    284\u001b[0m       sampler_state\u001b[38;5;241m.\u001b[39mdecoding_step \u001b[38;5;241m<\u001b[39m sampler_state\u001b[38;5;241m.\u001b[39mtotal_sampling_steps\n\u001b[1;32m    285\u001b[0m   ) \u001b[38;5;241m&\u001b[39m jnp\u001b[38;5;241m.\u001b[39many(jnp\u001b[38;5;241m.\u001b[39mlogical_not(sampler_state\u001b[38;5;241m.\u001b[39mdone))\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_with_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_sampling_state\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "File \u001b[0;32m~/gemma/gemma/sampler.py:280\u001b[0m, in \u001b[0;36mSampler._sample_fn.<locals>.sample_with_params\u001b[0;34m(sampler_state)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_with_params\u001b[39m(sampler_state: _SamplingState):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler_state\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/gemma/gemma/sampler.py:145\u001b[0m, in \u001b[0;36mSampler._sample_step\u001b[0;34m(self, params, sampler_state)\u001b[0m\n\u001b[1;32m    140\u001b[0m step_positions \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mexpand_dims(\n\u001b[1;32m    141\u001b[0m     sampler_state\u001b[38;5;241m.\u001b[39mpositions[:, decoding_step], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m last_token \u001b[38;5;241m=\u001b[39m last_token\u001b[38;5;241m.\u001b[39mreshape((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 145\u001b[0m logits, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampler_state\u001b[38;5;241m.\u001b[39mforbidden_token_ids:\n\u001b[1;32m    153\u001b[0m   logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mat[:, :, sampler_state\u001b[38;5;241m.\u001b[39mforbidden_token_ids]\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m-\u001b[39mjnp\u001b[38;5;241m.\u001b[39minf)\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/gemma/lib/python3.11/site-packages/flax/core/scope.py:1103\u001b[0m, in \u001b[0;36mapply.<locals>.wrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Try to detect if user accidentally passed {'params': {'params': ...}.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1099\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variables\n\u001b[1;32m   1100\u001b[0m   \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m], (\u001b[38;5;28mdict\u001b[39m, FrozenDict))\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1102\u001b[0m ):\n\u001b[0;32m-> 1103\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mApplyScopeInvalidVariablesStructureError(variables)\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m bind(\n\u001b[1;32m   1106\u001b[0m   variables, rngs\u001b[38;5;241m=\u001b[39mrngs, mutable\u001b[38;5;241m=\u001b[39mmutable, flags\u001b[38;5;241m=\u001b[39mflags\n\u001b[1;32m   1107\u001b[0m )\u001b[38;5;241m.\u001b[39mtemporary() \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m   1108\u001b[0m   y \u001b[38;5;241m=\u001b[39m fn(root, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mApplyScopeInvalidVariablesStructureError\u001b[0m: Expect the `variables` (first argument) passed to apply() to be a dict with the structure {\"params\": ...}, but got a dict with an extra params layer, i.e.  {\"params\": {\"params\": ... } }. You should instead pass in your dict's [\"params\"]. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesStructureError)"
          ]
        }
      ],
      "source": [
        "input_batch = [\n",
        "    \"\\n# Python program for implementation of Bubble Sort\\n\\ndef bubbleSort(arr):\",\n",
        "    \"What are the planets of the solar system?\",\n",
        "  ]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=300,  # number of steps performed when generating\n",
        "  )\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"Prompt:\\n{input_string}\\nOutput:\\n{out_string}\")\n",
        "  print()\n",
        "  print(10*'#')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "gemma",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
